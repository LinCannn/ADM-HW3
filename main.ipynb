{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting we import all the libraries that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myFunctions import *\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong> Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 1.1 Get the list of Michelin restaraunts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before scraping all the restaurant URLs, let's first determine the maximum page number. It's simple to find the correct CSS selector for the page list: just inspect the list of pages in your browser and identify the corresponding class or element name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img title = \"list of pages\" src=\"images/pages_number.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total: 102 pages\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://guide.michelin.com/en/it/restaurants')\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "page_links = soup.select('ul.pagination li a') #name of the pages list\n",
    "page_numbers = [int(a.get_text()) for a in page_links if a.get_text().isdigit()]\n",
    "\n",
    "# Get the maximum page number\n",
    "total_pages = max(page_numbers) if page_numbers else 0\n",
    "print(f'There are in total: {total_pages} pages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can very easily get the URL of each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = ['https://guide.michelin.com/en/it/restaurants'] #Initial page\n",
    "\n",
    "for i in range(2, total_pages+1): #get all other pages from 2 to total_pages included\n",
    "    pages.append('https://guide.michelin.com/en/it/restaurants/page/'+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to get the URLs of all the restaraunts, we proceed the same by identifying the name of the corresponding class in the webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img title = \"Class of a restaraunt\" src=\"images/restaurant_link.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the restaurant URLs follow a consistent pattern, which can be expressed using the regular expression:\n",
    "\n",
    "```bash\n",
    "BASE_URL/en/region/city/restaurant/name_of_restaurant\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_urls = [] #save all urls\n",
    "base = 'https://guide.michelin.com' #base url to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pages: #loop all pages\n",
    "    response = requests.get(p) #get the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\") # we use BeautifulSoup to get the content\n",
    "    links = soup.select('a.link') #select all the class 'a link'\n",
    "    pattern = re.compile(r'^/en/[^/]+/[^/]+/restaurant/[^/]+$') #pattern of restaurants\n",
    "    restaurant_links = [base+link.get('href') for link in links if pattern.match(link.get('href', ''))] #get all the restaurants links\n",
    "    total_urls.append(restaurant_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save all the urls inside a txt called 'restaurant_urls.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('restaurant_urls.txt', 'w') as f: \n",
    "    page_count = 1  # Initialize the page count\n",
    "    for urls in total_urls:\n",
    "        f.write(f'Page {page_count}:\\n')  # Add a label for the page number\n",
    "        for url in urls: # Write each URL from the current page\n",
    "            f.write(f'{url}\\n')  \n",
    "        \n",
    "        page_count += 1 # Increment the page count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2037\n"
     ]
    }
   ],
   "source": [
    "print(sum([len(u) for u in total_urls])) # how many restaurants we got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 1.2. Crawl Michelin restaurant pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we download all the HTML from the urls and save them in a folder and divide each of them in separate folder_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_all_as_html('restaurant_urls.txt') # See actual implementation inside 'myFunctions.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file count: 2034\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for root_dir, cur_dir, files in os.walk('restaurants_html'): #Let's check if we got all the files\n",
    "    count += len(files)\n",
    "print('file count:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The save_all_as_html function utilizes multi-threading to achieve optimal performance, generating approximately 20 threads concurrently. Within each loop for a page, each thread is tasked with downloading around a single URL, making it extremely efficient. Consequently, the function successfully downloaded 2,034 out of 2,037 files in under one minute."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
