{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting we import all the libraries that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyFunctions.crawler import Crawler\n",
    "from MyFunctions.parser import Parser\n",
    "from MyFunctions.preprocessor import Preprocessor\n",
    "from MyFunctions.searchEngine import SearchEngine, AdvancedSearchEngine\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong> Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 1.1 Get the list of Michelin restaraunts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before scraping all the restaurant URLs, let's first determine the maximum page number. It's simple to find the correct CSS selector for the page list: just inspect the list of pages in your browser and identify the corresponding class or element name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <img title = \"list of pages\" src=\"./images/pages_number.png\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total: 100 pages\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://guide.michelin.com/en/it/restaurants')\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "page_links = soup.select('ul.pagination li a') #name of the pages list\n",
    "page_numbers = [int(a.get_text()) for a in page_links if a.get_text().isdigit()]\n",
    "\n",
    "# Get the maximum page number\n",
    "total_pages = max(page_numbers) if page_numbers else 0\n",
    "print(f'There are in total: {total_pages} pages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can very easily get the URL of each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = ['https://guide.michelin.com/en/it/restaurants'] #Initial page\n",
    "\n",
    "for i in range(2, total_pages+1): #get all other pages from 2 to total_pages included\n",
    "    pages.append('https://guide.michelin.com/en/it/restaurants/page/'+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to get the URLs of all the restaurants, we proceed the same by identifying the name of the corresponding class in the webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<img title = \"Class of a restaraunt\" src=\"images/restaurant_link.png\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the restaurant URLs follow a consistent pattern, which can be expressed using the regular expression:\n",
    "\n",
    "```bash\n",
    "BASE_URL/en/region/city/restaurant/name_of_restaurant\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_urls = [] #save all urls\n",
    "base = 'https://guide.michelin.com' #base url to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pages: #loop all pages\n",
    "    response = requests.get(p) #get the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\") # we use BeautifulSoup to get the content\n",
    "    links = soup.select('a.link') #select all the class 'a link'\n",
    "    pattern = re.compile(r'^/en/[^/]+/[^/]+/restaurant/[^/]+$') #pattern of restaurants\n",
    "    restaurant_links = [base+link.get('href') for link in links if pattern.match(link.get('href', ''))] #get all the restaurants links\n",
    "    total_urls.append(restaurant_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save all the urls inside a txt called 'restaurant_urls.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/restaurant_urls.txt', 'w') as f: \n",
    "    page_count = 1  # Initialize the page count\n",
    "    for urls in total_urls:\n",
    "        f.write(f'{page_count}\\n')  # Add a label for the page number\n",
    "        for url in urls: # Write each URL from the current page\n",
    "            f.write(f'{url}\\n')  \n",
    "        \n",
    "        page_count += 1 # Increment the page count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1983\n"
     ]
    }
   ],
   "source": [
    "print(sum([len(u) for u in total_urls])) # how many restaurants we got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 1.2. Crawl Michelin restaurant pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we download all the HTML from the urls and save them in a folder and divide each of them in separate folder_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Crawler()\n",
    "crawler.save_all_as_html('dataset/restaurant_urls.txt') # See actual implementation inside 'crawler.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file count: 1983\n"
     ]
    }
   ],
   "source": [
    "path = 'restaurants_html'\n",
    "count = crawler.count_files(path)\n",
    "print('file count:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The save_all_as_html function utilizes multi-threading to achieve optimal performance, generating approximately 20 threads concurrently. Within each loop for a page, each thread is tasked with downloading around a single URL, making it extremely efficient. Consequently, the function successfully downloaded 2,034 out of 2,037 files in under one minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of the information we desire for each restaurant and their format is as follows:\n",
    "\n",
    "    Restaurant Name (to save as restaurantName): string;\n",
    "    Address (to save as address): string;\n",
    "    City (to save as city): string;\n",
    "    Postal Code (to save as postalCode): string;\n",
    "    Country (to save as country): string;\n",
    "    Price Range (to save as priceRange): string;\n",
    "    Cuisine Type (to save as cuisineType): string;\n",
    "    Description (to save as description): string;\n",
    "    Facilities and Services (to save as facilitiesServices): list of strings;\n",
    "    Accepted Credit Cards (to save as creditCards): list of strings;\n",
    "    Phone Number (to save as phoneNumber): string;\n",
    "    URL to the Restaurant Page (to save as website): string.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse those information we can just inspect one html to see how those information are stored as we did before.<br>\n",
    "Most of the information can be retrieved in the following json script at the end of each HTML file:\n",
    "```js\n",
    "<script type=\"application/ld+json\">{\"@context\":\"http://schema.org\",\"address\":{\"@type\":\"PostalAddress\",\"streetAddress\":\"Piazza Salvo d'Acquisto 16\",\"addressLocality\":\"Lamezia Terme\",\"postalCode\":\"88046\",\"addressCountry\":\"ITA\",\"addressRegion\":\"Calabria\"},\"name\":\"Abbruzzino Oltre\",\"image\":\"https://axwwgrkdco.cloudimg.io/v7/__gmpics3__/f19d37d6b9da437fa06b6f9406645056.jpg?width=1000\",\"@type\":\"Restaurant\",\"review\":{\"@type\":\"Review\",\"datePublished\":\"2024-09-11T07:32\",\"name\":\"Abbruzzino Oltre\",\"description\":\"This restaurant, the new home of young chef Luca Abbruzzino, occupies the first floor of a historic palazzo in the town centre which has recently been converted into a small hotel offering six ...\",\"author\":{\"@type\":\"Person\",\"name\":\"Michelin Inspector\"}},\"telephone\":\"+39 0968 188 8038\",\"knowsLanguage\":\"en-IT\",\"acceptsReservations\":\"No\",\"servesCuisine\":\"Contemporary\",\"url\":\"https://guide.michelin.com/en/calabria/lamezia-terme/restaurant/abbruzzino-oltre\",\"currenciesAccepted\":\"EUR\",\"paymentAccepted\":\"American Express credit card, Credit card / Debit card accepted, Mastercard credit card, Visa credit card\",\"award\":\"Selected: Good cooking\",\"brand\":\"MICHELIN Guide\",\"hasDriveThroughService\":\"False\",\"latitude\":38.9770969,\"longitude\":16.3202202,\"hasMap\":\"https://www.google.com/maps/search/?api=1&query=38.9770969%2C16.3202202\"}</script>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/restaurant_page.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a parse_restaurant function that given a html, it parses all the information we need and returns it as a dictionary, we also decided to keep region as an extra column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurantName: La Trattoria Enrico Bartolini\n",
      "address: Località Badiola\n",
      "city: Castiglione della Pescaia\n",
      "postalCode: 58043\n",
      "country: ITA\n",
      "region: Tuscany\n",
      "priceRange: €€€€\n",
      "cuisineType: Mediterranean Cuisine, Grills\n",
      "description: After a majestic picture-postcard approach via a long avenue lined with cypress trees and maritime pines, passing vineyards and Maremma cattle along the way, you finally arrive at this restaurant which serves trattoria-style cuisine full of intense, familiar and reassuring flavours. The decor here is elegant with the occasional rustic touch, while the service is of the highest level yet pleasantly friendly and informal. Welcome to Bartolini’s Maremma restaurant! Here, resident chef Bruno De Moura Cossio offers a choice of dishes with one common denominator, namely charcoal grilling. All the dishes served here have been grilled in some way, so that they have a distinctive barbecued flavour. However, although the chef’s Brazilian origins are obvious in many different ways, the ingredients are resolutely local, some even from the restaurant’s own farm. The friendly service makes guests feel completely at home, while the excellent wine selection is an added attraction – those produced on the estate itself are highly recommended.\n",
      "facilitiesServices: ['Air conditioning', 'Car park', 'Garden or park', 'Interesting wine list', 'Terrace']\n",
      "creditCards: ['Amex', 'Mastercard', 'Visa']\n",
      "phoneNumber: +39 0564 944322\n",
      "website: https://www.enricobartolini.net/ristorante-la-trattoria-castiglione\n"
     ]
    }
   ],
   "source": [
    "parser = Parser()\n",
    "info = parser.parse_restaurant('restaurants_html/1/la-trattoria-enrico-bartolini.html') #Test\n",
    "parser.show_restaurant_info(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a tsv file with all the informations of all the restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to dataset/restaurant_info.tsv\n"
     ]
    }
   ],
   "source": [
    "root = 'restaurants_html'\n",
    "output= 'dataset/restaurant_info.tsv'\n",
    "parser.save_all_restaurant_info_to_tsv(root, output) #actual implementation in Parser class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table('dataset/restaurant_info.tsv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong> Search Engine </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 2.0.0. Preprocessing the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the search engine, we need to prepare and clean the restaurant descriptions in our dataset. To accomplish this, we created a class named Preprocessor in preprocessor.py. This class leverages the nltk library to process the text in the description column. It removes stopwords and punctuation, converts the text to lowercase, and applies stemming to reduce words to their base forms. This preprocessing step ensures that the descriptions are standardized, making them more suitable for efficient search and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pavka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/pavka/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_table('dataset/restaurant_info.tsv')\n",
    "preprocessor = Preprocessor()\n",
    "df = preprocessor.filter(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's drop the column of description as we don't need it anymore and save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='description',inplace=True)\n",
    "df.to_csv('dataset/restaurant_info_filtered.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 2.1. Conjunctive Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"dataset/restaurant_info_filtered.tsv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 2.1.1. Create Your Index!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a vocabulary that maps each word to a unique integer (term_id) and save it in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descriptions = df['description_filtered'].str.cat(sep=' ')\n",
    "all_descriptions = list(set(all_descriptions.split(\" \")))\n",
    "vocabulary = {word:id for id, word in enumerate(all_descriptions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this vocabulary to a file with utf-8 encoding in order to be able to handle all the characters\n",
    "with open('dataset/vocabulary.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['word', 'term_id'])  # Header\n",
    "    for word, id in vocabulary.items():\n",
    "        writer.writerow([word, id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an inverted index and save it into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pd.read_csv(\"dataset/vocabulary.csv\")\n",
    "df = pd.read_table(\"dataset/restaurant_info_filtered.tsv\")\n",
    "df = df[['restaurantName','description_filtered']]\n",
    "df['description_filtered'] = df['description_filtered'].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_restaurants = defaultdict(list)\n",
    "\n",
    "# Iterate through the restaurant descriptions and update the dictionary\n",
    "for i, row in df.iterrows():\n",
    "    for word in row['description_filtered']:\n",
    "        word_to_restaurants[word].append(row['restaurantName'])\n",
    "\n",
    "# Add the 'restaurants_containing_word' column to the vocabulary DataFrame\n",
    "vocabulary['restaurants_containing_word'] = vocabulary['word'].apply(lambda x: word_to_restaurants[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>term_id</th>\n",
       "      <th>restaurants_containing_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mattiacci</td>\n",
       "      <td>0</td>\n",
       "      <td>[La Gioconda]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>basil</td>\n",
       "      <td>1</td>\n",
       "      <td>[Il Buco, Aria, Regallo, La Caravella dal 1959...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>betray</td>\n",
       "      <td>2</td>\n",
       "      <td>[Albergaccio di Castellina]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alway</td>\n",
       "      <td>3</td>\n",
       "      <td>[Dal Pescatore, Osteria dell'Arco, Marcelin, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mascia</td>\n",
       "      <td>4</td>\n",
       "      <td>[San Domenico]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7780</th>\n",
       "      <td>discreetli</td>\n",
       "      <td>7780</td>\n",
       "      <td>[Morelli, Marelet, GioEle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7781</th>\n",
       "      <td>peopl</td>\n",
       "      <td>7781</td>\n",
       "      <td>[Hosteria Giusti, Franceschetta 58, Taverna Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7782</th>\n",
       "      <td>sebada</td>\n",
       "      <td>7782</td>\n",
       "      <td>[Sa Domu Sarda]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7783</th>\n",
       "      <td>florian</td>\n",
       "      <td>7783</td>\n",
       "      <td>[Umberto De Martino, Castel fine dining]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7784</th>\n",
       "      <td>pergola-shad</td>\n",
       "      <td>7784</td>\n",
       "      <td>[Trattoria della Fortuna]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7785 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  term_id                        restaurants_containing_word\n",
       "0        mattiacci        0                                      [La Gioconda]\n",
       "1            basil        1  [Il Buco, Aria, Regallo, La Caravella dal 1959...\n",
       "2           betray        2                        [Albergaccio di Castellina]\n",
       "3            alway        3  [Dal Pescatore, Osteria dell'Arco, Marcelin, A...\n",
       "4           mascia        4                                     [San Domenico]\n",
       "...            ...      ...                                                ...\n",
       "7780    discreetli     7780                         [Morelli, Marelet, GioEle]\n",
       "7781         peopl     7781  [Hosteria Giusti, Franceschetta 58, Taverna Ro...\n",
       "7782        sebada     7782                                    [Sa Domu Sarda]\n",
       "7783       florian     7783           [Umberto De Martino, Castel fine dining]\n",
       "7784  pergola-shad     7784                          [Trattoria della Fortuna]\n",
       "\n",
       "[7785 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save inverted index into a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {term_id:rs for term_id, rs in zip(vocabulary['term_id'], vocabulary['restaurants_containing_word'])}\n",
    "with open('dataset/inverted_index.json', 'w') as jsonfile:\n",
    "    json.dump(inverted_index, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 2.1.2. Execute the Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "originale = \"dataset/restaurant_info.tsv\"\n",
    "df = \"dataset/restaurant_info_filtered.tsv\"\n",
    "vocabulary = \"dataset/vocabulary.csv\"\n",
    "inverted_index = \"dataset/inverted_index.json\"\n",
    "\n",
    "searcher = SearchEngine(originale, df,vocabulary,inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurantName</th>\n",
       "      <th>address</th>\n",
       "      <th>description</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Hyle</td>\n",
       "      <td>contrada Torre Garga SS 107</td>\n",
       "      <td>Located on the wooded Sila plateau famous sinc...</td>\n",
       "      <td>https://www.hyleristorante.it/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Eea</td>\n",
       "      <td>via Umberto I</td>\n",
       "      <td>Named after the Ancient Greek name for Ponza, ...</td>\n",
       "      <td>https://www.mondoeea.it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>Taverna Kerkira</td>\n",
       "      <td>corso Vittorio Emanuele 217</td>\n",
       "      <td>Kerkira (Corfu in Greek) owes its existence to...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>50 Kalò</td>\n",
       "      <td>piazza Sannazzaro 201/b</td>\n",
       "      <td>In the Neapolitan tradition of interpreting dr...</td>\n",
       "      <td>https://www.50kalo.it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>Il Pievano</td>\n",
       "      <td>Località Spaltenna 13</td>\n",
       "      <td>Occupying the charming, romantic setting of a ...</td>\n",
       "      <td>https://www.spaltenna.it/it/ristoranti/il-pievano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>Palais Royal Restaurant</td>\n",
       "      <td>Calle Larga 22 Marzo 2032</td>\n",
       "      <td>This restaurant is an Italian offshoot of the ...</td>\n",
       "      <td>https://nolinskivenezia.com/it/palais-royal-re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               restaurantName                      address  \\\n",
       "193                      Hyle  contrada Torre Garga SS 107   \n",
       "269                       Eea                via Umberto I   \n",
       "985           Taverna Kerkira  corso Vittorio Emanuele 217   \n",
       "1223                  50 Kalò      piazza Sannazzaro 201/b   \n",
       "1484               Il Pievano        Località Spaltenna 13   \n",
       "1814  Palais Royal Restaurant    Calle Larga 22 Marzo 2032   \n",
       "\n",
       "                                            description  \\\n",
       "193   Located on the wooded Sila plateau famous sinc...   \n",
       "269   Named after the Ancient Greek name for Ponza, ...   \n",
       "985   Kerkira (Corfu in Greek) owes its existence to...   \n",
       "1223  In the Neapolitan tradition of interpreting dr...   \n",
       "1484  Occupying the charming, romantic setting of a ...   \n",
       "1814  This restaurant is an Italian offshoot of the ...   \n",
       "\n",
       "                                                website  \n",
       "193                      https://www.hyleristorante.it/  \n",
       "269                             https://www.mondoeea.it  \n",
       "985                                                 NaN  \n",
       "1223                              https://www.50kalo.it  \n",
       "1484  https://www.spaltenna.it/it/ristoranti/il-pievano  \n",
       "1814  https://nolinskivenezia.com/it/palais-royal-re...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ideal_restaurants = searcher.search(\"greek\")\n",
    "ideal_restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong> 2.2. Ranked Search Engine with TF-IDF and Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 2.2.1 Inverted Index with TF-IDF Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 1: Query TF-IDF Calculations**\n",
    "| Word | Word Count | Total Words | TF | IDF Calculation | IDF Value | TF-IDF Score |\n",
    "|------|------------|-------------|-----|-----------------|-----------|--------------|\n",
    "| modern | 1 | 3 | 1/3 = 0.333 | log(100/1) | 4.605 | 0.333 × 4.605 = 1.534 |\n",
    "| seasonal | 1 | 3 | 1/3 = 0.333 | log(100/1) | 4.605 | 0.333 × 4.605 = 1.534 |\n",
    "| cuisine | 1 | 3 | 1/3 = 0.333 | log(100/1) | 4.605 | 0.333 × 4.605 = 1.534 |\n",
    "\n",
    "Query Vector = [1.534, 1.534, 1.534]\n",
    "Query Vector Magnitude = √(1.534² + 1.534² + 1.534²) = 2.657\n",
    "\n",
    "**Table 2: Document TF-IDF Calculations**\n",
    "| Word | Word Count | Total Words | TF | IDF Calculation | IDF Value | TF-IDF Score |\n",
    "|------|------------|-------------|-----|-----------------|-----------|--------------|\n",
    "| modern | 2 | 74 | 2/74 = 0.027 | log(100/1) | 4.605 | 0.027 × 4.605 = 0.124 |\n",
    "| seasonal | 1 | 74 | 1/74 = 0.014 | log(100/1) | 4.605 | 0.014 × 4.605 = 0.064 |\n",
    "| cuisine | 0 | 74 | 0/74 = 0 | log(100/1) | 4.605 | 0 × 4.605 = 0 |\n",
    "\n",
    "Document Vector = [0.124, 0.064, 0]\n",
    "Document Vector Magnitude = √(0.124² + 0.064² + 0²) = 0.139\n",
    "\n",
    "**Cosine Similarity Calculation:**\n",
    "```\n",
    "Dot Product = (1.534 × 0.124) + (1.534 × 0.064) + (1.534 × 0)\n",
    "            = 0.190 + 0.098 + 0\n",
    "            = 0.288\n",
    "\n",
    "Cosine Similarity = 0.288 / (2.657 × 0.139)\n",
    "                  = 0.288 / 0.369\n",
    "                  = 0.780\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create the tf() function, given any string, it outputs the frequency of each word in the string in a list, for example query = \" modern searsonal, cuisine\"  - > (modern: 1/3, seasonal 1/3, cuisine 1/3), this tf function should only get the words inside the query for each description so d1 = [1/40, 2/40,1/40] if n_words inside d1 is 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_index_dict(query):\n",
    "    words = re.findall(r'\\w+', query.lower())  # Use regex to only pick up word characters\n",
    "    word_index = {word: idx for idx, word in enumerate(set(words))}\n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(query):\n",
    "    # First, create the word index dictionary\n",
    "    index_dict = word_index_dict(query)\n",
    "    print(index_dict)\n",
    "    \n",
    "    # Clean and split the query into words\n",
    "    words = re.findall(r'\\w+', query.lower())  # Use regex to only pick up word characters\n",
    "    total_words = len(words)\n",
    "    \n",
    "    # Calculate the frequency of each word\n",
    "    word_count = Counter(words)\n",
    "    \n",
    "    # Create a list of term frequencies (word: count / total_words)\n",
    "    word_tf = {word: count / total_words for word, count in word_count.items()}\n",
    "    \n",
    "    return word_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seasonal': 0, 'cuisine': 1, 'modern': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'modern': 0.3333333333333333,\n",
       " 'seasonal': 0.3333333333333333,\n",
       " 'cuisine': 0.3333333333333333}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the tf function\n",
    "query = 'modern seasonal cuisine'\n",
    "word_tf = tf(query)\n",
    "word_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. create a global idf score, this score is the same for query and descriptions, so for each term possible do the formula and at the end just get the idf scores of the  words in the query (we dont need every single score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df = pd.read_table(\"dataset/restaurant_info_filtered.tsv\")\n",
    "documents = df['description_filtered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(documents):\n",
    "    num_descriptions = len(documents) # total numbers of descriptions\n",
    "    word_document_counts = Counter()\n",
    "\n",
    "    # Count the number of documents each word appears in\n",
    "    for doc in documents:\n",
    "        unique_words = set(re.findall(r'\\w+', doc.lower()))  # Use set to avoid double-counting words in the same document\n",
    "        for word in unique_words:\n",
    "            word_document_counts[word] += 1\n",
    "\n",
    "    # Calculate IDF for each word\n",
    "    idf_scores = {}\n",
    "    for word, df in word_document_counts.items():\n",
    "        idf_scores[word] = math.log(num_descriptions / df)\n",
    "\n",
    "    return idf_scores\n",
    "\n",
    "idf_scores = idf(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_and_idf_query(query, idf_scores):\n",
    "    # First, calculate the TF scores for the query\n",
    "    tf_query = word_tf\n",
    "\n",
    "    # Now, for each word in the query, get the IDF score (if it exists in the global IDF dictionary)\n",
    "    tf_idf_scores_query = {}\n",
    "    for word, tf in tf_query.items():\n",
    "        idf = idf_scores.get(word, 0)  # Default to 0 if the word is not in the IDF dictionary\n",
    "        tf_idf_scores_query[word] = tf * idf\n",
    "\n",
    "    return tf_query, tf_idf_scores_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Scores: {'modern': 0.3333333333333333, 'seasonal': 0.3333333333333333, 'cuisine': 0.3333333333333333}\n",
      "TF-IDF Scores: {'modern': 0.4519988461053638, 'seasonal': 0.0, 'cuisine': 2.5307887095065986}\n"
     ]
    }
   ],
   "source": [
    "# Testing tf_and_idf_query function\n",
    "query = \"modern seasonal cuisine\"\n",
    "tf_query, tf_idf_scores_query = tf_and_idf_query(query, idf_scores)\n",
    "print(\"TF Scores:\", tf_query)\n",
    "print(\"TF-IDF Scores:\", tf_idf_scores_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documents_tf(documents):\n",
    "    tf_document = {}\n",
    "    for doc in documents:\n",
    "        word_counts = Counter(re.findall(r'\\w+', doc.lower()))\n",
    "        total_words = len(word_counts)\n",
    "        tf_document[doc] = {word: count / total_words for word, count in word_counts.items()}\n",
    "    return tf_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_document = documents_tf(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_and_idf_documents(documents, idf_scores):\n",
    "    tf_scores = tf_document\n",
    "    \n",
    "    # Now calculate TF-IDF for each description\n",
    "    description_tf_idf_scores = {}\n",
    "    for doc, tf in tf_scores.items():\n",
    "        doc_idf_scores = {word: idf_scores.get(word, 0) for word in tf}\n",
    "        doc_tf_idf_scores = {word: tf[word] * doc_idf_scores[word] for word in tf}\n",
    "        description_tf_idf_scores[doc] = doc_tf_idf_scores\n",
    "    return description_tf_idf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_tf_idf_scores = tf_and_idf_documents(documents, idf_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. given the query_idf_score of 1x3, for the example of \"modern seasonal cuisine\", do the element-wise multiplication to each tf-score of the descriptions and query, so now u have the tf-idf score of each description and the query, all of them of size 1x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. just do cosine similarity with vector1 being always the query_score and vector2 being different descriptions score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarities(tf_idf_scores_query, description_tf_idf_scores):\n",
    "    # Convert lists to arrays for cosine similarity calculation\n",
    "    tf_idf_scores_query = np.array(tf_idf_scores_query).reshape(1, -1)\n",
    "    description_tf_idf_scores = np.array(description_tf_idf_scores)\n",
    "    \n",
    "    # Calculate cosine similarity between the query and each document\n",
    "    similarities = cosine_similarity(tf_idf_scores_query, description_tf_idf_scores)[0]\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_idf_scores_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription_tf_idf_scores\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[43], line 7\u001b[0m, in \u001b[0;36mcosine_similarities\u001b[1;34m(tf_idf_scores_query, description_tf_idf_scores)\u001b[0m\n\u001b[0;32m      4\u001b[0m description_tf_idf_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(description_tf_idf_scores)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarity between the query and each document\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_idf_scores_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription_tf_idf_scores\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m similarities\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\pairwise.py:1679\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1636\u001b[0m \n\u001b[0;32m   1637\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1675\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1679\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1681\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\pairwise.py:185\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    175\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    176\u001b[0m         X,\n\u001b[0;32m    177\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 185\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m     Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    195\u001b[0m         Y,\n\u001b[0;32m    196\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    202\u001b[0m     )\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_array_api.py:745\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    743\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 745\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'dict'"
     ]
    }
   ],
   "source": [
    "similarities = cosine_similarities(tf_idf_scores_query, description_tf_idf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(tsimilarities):\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    similarities = cosine_similarities(query_tf_idf, document_tf_idfs)\n",
    "    \n",
    "    # Add the similarities as a new column in the DataFrame\n",
    "    df['similarity score'] = similarities\n",
    "    return df.sort_values(by='similarity score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in short, just create a tf function that gives back the correct tf-vector given a string, IMPORTANT, EACH INDEX SHOULD REPRESENT A SPECIFIC WORD, if you calculated modern-tf in index 0 of the query list, all other documents should put the modern-tf in index 0, the other steps are trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_scores = searcher.get_restaurant_scores(\"pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 2.2.2. Execute the Ranked Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong> 3. Define a New Score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <strong> Steps:<br>\n",
    "    - User Query: The user provides a text query. We’ll retrieve relevant documents using the search engine built in Step 2.1.<br>\n",
    "    - New Ranking Metric: After retrieving relevant documents, we’ll rank them using a new custom score. Instead of limiting the scoring to only the description field, we can include other attributes like priceRange, facilitiesServices, and cuisineType.<br>\n",
    "    - You will use a heap data structure (e.g., Python’s heapq library) to maintain the top-k restaurants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"dataset/restaurant_info.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong> 5. BONUS: Advanced Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 5.1 - Specify Search Criteria: Users can specify search terms for the following features (any or all of them):\n",
    "    - restaurantName\n",
    "    - city\n",
    "    - cuisineType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create the inverted index for each of the features, we can do it very easily by creating a function and repeat the function call for each column we want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"dataset/restaurant_info.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createIndex(df, column):\n",
    "    # Convert all unique words to lowercase and apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    df[column] = df[column].fillna('na')\n",
    "    all_words = pd.Series(\" \".join(df[column].str.lower()).split()).unique()\n",
    "    all_words_stemmed = [stemmer.stem(word) for word in all_words]\n",
    "    \n",
    "    # Initialize the dictionary for stemmed words\n",
    "    word_to_restaurants = {word: [] for word in all_words_stemmed}\n",
    "    \n",
    "    # Split each row's text, stem each word, and update the index\n",
    "    for i, row in df.iterrows():\n",
    "        words = row[column].lower().split()\n",
    "        for word in words:\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            word_to_restaurants[stemmed_word].append(row['restaurantName'])\n",
    "    \n",
    "    # Save to JSON\n",
    "    path = f'dataset/{column}_index.json'\n",
    "    with open(path, 'w') as jsonfile:\n",
    "        json.dump(word_to_restaurants, jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createIndex(df, 'restaurantName')\n",
    "createIndex(df, 'city')\n",
    "createIndex(df, 'cuisineType')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 5.2 - Price Range Filter: Allow users to set a price range (e.g., between € and €€€) to filter the results by affordability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "createIndex(df, 'priceRange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 5.3 - Region Filter: Enable users to specify a list of Italian regions to limit the search to restaurants within those regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "createIndex(df, 'region')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 5.4 - Accepted Credit Cards: Provide an option to filter by accepted credit card types. Users can specify one or more preferred card types (e.g., Visa, MasterCard, Amex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "createIndex(df, 'creditCards')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 5.5 - Services and Facilities: Allow users to filter based on specific services and facilities provided by the restaurant. For example, users may look for amenities like Wi-Fi, Terrace, Air Conditioning, or Parking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "createIndex(df, 'facilitiesServices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> Now we can just implement use the AdvancedSearchEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurantName</th>\n",
       "      <th>address</th>\n",
       "      <th>cuisineType</th>\n",
       "      <th>priceRange</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Angiò-Macelleria di Mare</td>\n",
       "      <td>viale Africa 28/h</td>\n",
       "      <td>Seafood</td>\n",
       "      <td>€€</td>\n",
       "      <td>https://albertoangiolucci.it/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>Sapio</td>\n",
       "      <td>piazza Antonino Gandolfo 11</td>\n",
       "      <td>Sicilian, Modern Cuisine</td>\n",
       "      <td>€€€€</td>\n",
       "      <td>https://www.sapiorestaurant.it/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Me Cumpari Turiddu</td>\n",
       "      <td>piazza Turi Ferro 36</td>\n",
       "      <td>Sicilian</td>\n",
       "      <td>€</td>\n",
       "      <td>https://www.mecumparituriddu.it/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>Ménage</td>\n",
       "      <td>Via Euplio Reina 13</td>\n",
       "      <td>Sicilian, Contemporary</td>\n",
       "      <td>€€</td>\n",
       "      <td>https://www.menagelounge.it/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>Concezione Restaurant</td>\n",
       "      <td>via Giuseppe Verdi 143</td>\n",
       "      <td>Creative, Sicilian</td>\n",
       "      <td>€€€</td>\n",
       "      <td>https://concezionerestaurant.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>Materia | Spazio Cucina</td>\n",
       "      <td>via Teatro Massimo 29</td>\n",
       "      <td>Sicilian, Modern Cuisine</td>\n",
       "      <td>€€</td>\n",
       "      <td>https://www.materiaspaziocucina.it/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>Coria</td>\n",
       "      <td>Via Prefettura 21</td>\n",
       "      <td>Italian Contemporary, Sicilian</td>\n",
       "      <td>€€€</td>\n",
       "      <td>https://www.ristorantecoria.it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                restaurantName                      address  \\\n",
       "12    Angiò-Macelleria di Mare            viale Africa 28/h   \n",
       "563                      Sapio  piazza Antonino Gandolfo 11   \n",
       "996         Me Cumpari Turiddu         piazza Turi Ferro 36   \n",
       "1054                    Ménage          Via Euplio Reina 13   \n",
       "1550     Concezione Restaurant       via Giuseppe Verdi 143   \n",
       "1779   Materia | Spazio Cucina        via Teatro Massimo 29   \n",
       "1934                     Coria            Via Prefettura 21   \n",
       "\n",
       "                         cuisineType priceRange  \\\n",
       "12                           Seafood         €€   \n",
       "563         Sicilian, Modern Cuisine       €€€€   \n",
       "996                         Sicilian          €   \n",
       "1054          Sicilian, Contemporary         €€   \n",
       "1550              Creative, Sicilian        €€€   \n",
       "1779        Sicilian, Modern Cuisine         €€   \n",
       "1934  Italian Contemporary, Sicilian        €€€   \n",
       "\n",
       "                                  website  \n",
       "12          https://albertoangiolucci.it/  \n",
       "563       https://www.sapiorestaurant.it/  \n",
       "996      https://www.mecumparituriddu.it/  \n",
       "1054         https://www.menagelounge.it/  \n",
       "1550    https://concezionerestaurant.com/  \n",
       "1779  https://www.materiaspaziocucina.it/  \n",
       "1934       https://www.ristorantecoria.it  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advanced = AdvancedSearchEngine()\n",
    "query = dict(\n",
    "    city = 'catania',\n",
    "    region = 'sicily'\n",
    ")\n",
    "advanced.search(**query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
